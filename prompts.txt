MAIN LLM INSTANCE:
{
 "prompt": "You control a robot by selecting its next operating mode based on the current context. **You try to do the task that is mentioned**",
 "context": {
  "modes": {
   "SPEECH": "Only possible nearby a Human below 1.5m distance",
   "SCAN": "Scan environment fo find known_objects",
   "NAVIGATE": "Move to known_objects",
   "PICKUP": "Grab known_object, needs to be below 1.5m distance",
   "PLACE": "Put down holding_object, needs to be below 1.5m distance at wished location"
  },
  "task": "Find a human and ask him",
  "modes_history": "None",
  "known_objects": [
   [
    "None",
    "None"
   ]
  ],
  "location_robot": "not_near_any_object",
  "holding_object": "None"
 },
 "instructions": "REPLY IN expected_response JSON FORMAT, ANSWER SUPER SHORT, before writing the JSON, make a CHAIN-OF-THOUGHT of your plan and why it makes sense, consider the distances and the implications/shortcomings your choices have.",
 "mode_feedback": "",
 "expected_response": {
  "reasoning": "brief_explanation_based_on_the_task",
  "next_mode": "selected_mode",
  "target_object": "None or navigation_specific_object"
 }
}

SCAN VIT QUESTION and LLM INSTANCE:
vit_results = call_vit_service(question="What happens in the image and what objects can you see? SUPER SHORT ANSWER")
{
 "prompt": "Extract all key objects(mention humans, man, woman also in objects) and environments from the Vision Transformer's scene description. Focus on common objects and spaces in a room.",
 "context": {"vit_results": vit_results},
 "instructions": "Reason through what would make the most sense which objects/spaces are needed. Convert synonyms to common terms (e.g., woman/man to human) ANSWER SHORT",
 "expected_response": {
  "reasoning": "brief_explanation",
  "detect_objects": "[\"relevant_objects_and_humans\"]",
  "detect_space": "[\"relevant_higher_level_spaces(e.g.kitchen/bedroom/etc\"]"
 }
}

SPEECH VIT Question and LLM INSTANCE:
vit_results = call_vit_service(question="Do the humans look towards the camera?")
{
 "prompt": "Determine if human is actively ready to interact based on visual cues. Use the results from a ViT.",
 "context": {"vit_results": vit_results},
 "instructions": "Reason through what would make the most sense, decide if you should start speaking with them, if yes, ask them for a task. The human needs to look at you actively! ANSWER SHORT",
 "expected_response": {
  "reasoning": "brief_explanation",
  "speech": "only_yes_or_no",
  "get_attention": "only_yes_or_no",
  "question": "potential_question"
 }
}

